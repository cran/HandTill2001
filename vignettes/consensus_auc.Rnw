%\VignetteIndexEntry{Consenus models weighted by multiclass AUC}
\documentclass[a4paper]{article}
\bibliographystyle{apalike}
\usepackage[utf8]{inputenc}
\title{Consensus models weighted by AUC for multiple class responses}
\author{Andreas Dominik Cullmann}

\SweaveOpts{echo=FALSE}
\SweaveOpts{print=FALSE}
\SweaveOpts{width=60}
%\VignetteIndexEntry{Consenus models}
\begin{document}
\maketitle
\providecommand{\rpack}[1]{package \texttt{#1}}
\providecommand{\rdata}[1]{\texttt{#1} data}
\providecommand{\rcode}[1]{\texttt{#1}}
\section{Introduction}
This vignette shows how to build a consensus model for the \rdata{fgl} (see \cite{MASS} or \rcode{?MASS::fgl}).
We will follow the modelling process shown in \cite[Figure 1]{marmion2009} restricting ourselves to only two 'Single-models': a classification tree and a multinomial log-linear model using \rpack{rpart} and \rpack{nnet}, respectively.
\section{Example}
<<echo=false,print=false>>=
options(useFancyQuotes="UTF-8")
@
After loading the data, we add random indices for training and evaluation sets (since the training and evaluation sets are complements with respect to the data set, two indices are redundant, but \rcode{subset(fgl, ind.eval)} might be easier to read than \rcode{subset(fgl, !ind.train)}).
<<echo=true>>=
library(MASS)
data(fgl)
set.seed(100)
ind.train <- sample(nrow(fgl)
                   , size = floor(nrow(fgl)*.7)
                   , replace = FALSE
                   )
ind.eval <- setdiff(seq(1:nrow(fgl)), ind.train)
fgl$ind.eval <- FALSE; fgl$ind.eval[ind.eval] <- TRUE
fgl$ind.train <- FALSE; fgl$ind.train[ind.train] <- TRUE
@
Choosing \rcode{fgl\$type} as response and all other variables as predictors, we calibrate a classification tree using \rcode{rpart::rpart} (cf. \cite[p. 264]{MASS}): 
<<echo=true>>=
library(rpart)
set.seed(123)
fgl.rpart <- rpart(type ~ .
                 , data = subset(fgl, ind.train)
                 , parms = list(split = "information")
                 )

newcp <- max(fgl.rpart$cptable[,"CP"] *
	     as.numeric(fgl.rpart$cptable[,"xerror"] <
			sum(fgl.rpart$cptable[dim(fgl.rpart$cptable)[1]
                                              , c("xerror","xstd")]))
	     ) + 1e-13
fgl.rpart.pruned <- prune(fgl.rpart, cp = newcp)
@
and a multinomial log-linear model using \rcode{nnet::multinom}:
<<echo=true>>=
library(nnet)
fgl.multinom <- multinom(type ~ .
                            , data = subset(fgl, ind.train)
			    , trace = FALSE
                            )
@
We can now assess the model accuracy using either a confusion matrix of the classified predictions
<<echo=true>>=
library(mda)
confusion(subset(fgl, ind.eval)$type
	  , predict(fgl.rpart.pruned
		    , newdata = subset(fgl, ind.eval)
		    , type = "class"))
confusion(subset(fgl, ind.eval)$type
	  , predict(fgl.multinom
		    , newdata = subset(fgl, ind.eval)
		    , type = "class"))
@
or a multiple class version of AUC using the raw predictions:
<<echo=true>>=
library(HandTill2001)
auc(multcap(response = subset(fgl, ind.eval)$type
	     , predicted = predict(fgl.rpart.pruned
				   , newdata = subset(fgl, ind.eval))
	     ))

auc(multcap(response = subset(fgl, ind.eval)$type
	     , predicted = predict(fgl.multinom
				   , newdata = subset(fgl, ind.eval)
				   , type = "probs"
				   )
	     ))
@
To enhance predictive performance, we decide to use both models to build a consenus model. 
Furthermore, we want to use the 'weighted average' consensus method given by \cite[Eqn 1]{marmion2009}, which uses the pre-evaluated AUC of the models as weights.
So we split the training set into two complementary subsets: 'inner training' and 'inner evaluation'.
<<echo=true>>=
set.seed(100)
ind.inner.train <- sample(ind.train
                         , size = floor(length(ind.train)*0.7)
                         , replace = FALSE
                         )
ind.inner.eval <- setdiff(ind.train, ind.inner.train)
fgl$ind.inner.eval <- FALSE; fgl$ind.inner.eval[ind.inner.eval] <- TRUE
fgl$ind.inner.train <- FALSE; fgl$ind.inner.train[ind.inner.train] <- TRUE
@
We then refit our two models to the 'inner training' data:
<<echo=true>>=
wa.fgl.multinom <- multinom(fgl.multinom
                            , data = subset(fgl, ind.inner.train)
			    , trace = FALSE
                            )
wa.fgl.rpart <- rpart(type ~ .
                     , data = subset(fgl, ind.inner.train)
                     , parms = list(split = "information")
                     )
newcp <- max(wa.fgl.rpart$cptable[,"CP"] *
             as.numeric(wa.fgl.rpart$cptable[,"xerror"] <
                        sum(wa.fgl.rpart$cptable[dim(wa.fgl.rpart$cptable)[1]
                                                , c("xerror","xstd")]))
             ) + 1e-13
wa.fgl.rpart.pruned <- prune(wa.fgl.rpart, cp = newcp)
@
and calculate pre-evaluation AUC (which we save in a \rcode{list}) using the 'inner evaluation' data and the refitted models:
<<echo=true>>=
li <- list()
li$rpart$auc <- auc(multcap(response = subset(fgl, ind.inner.eval)$type
                             , predicted = predict(wa.fgl.rpart.pruned
                                 , newdata = subset(fgl
                                     , ind.inner.eval))))
li$mllm$auc <- auc(multcap(response = subset(fgl, ind.inner.eval)$type
                            , predicted = predict(wa.fgl.multinom
                                , newdata = subset(fgl
                                    , ind.inner.eval)
                                , type = "probs")))
@
We add the predictions using the models (the 'original' or 'Single' ones, not the refitted) for the evaluation set
<<echo=true>>=
li$rpart$predictions <- predict(fgl.rpart.pruned
                                , newdata = subset(fgl, ind.eval))
li$mllm$predictions <- predict(fgl.multinom
                               , newdata = subset(fgl, ind.eval)
                               , type = "probs")
@
and obtain the consensus predictions as (\cite[Eqn 1]{marmion2009})
<<echo=true>>=
predicted <- Reduce('+', lapply(li, function(x)
				x$auc * x$predictions)
) / Reduce('+', sapply(li,"[", "auc"))
@
which perform (slightly) better than the predictions using the 'single models':
<<echo=true>>=
auc(multcap(response= subset(fgl, ind.eval)$type
             , predicted = predicted)
     )
@
\bibliography{bibliography}
\end{document}
